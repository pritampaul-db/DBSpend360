{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712232c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-identity azure-mgmt-costmanagement azure.mgmt.costmanagement\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d3ce36",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.mgmt.costmanagement import CostManagementClient\n",
    "from azure.mgmt.costmanagement.models import (\n",
    "    QueryDefinition,\n",
    "    QueryDataset,\n",
    "    QueryTimePeriod,\n",
    "    QueryAggregation,\n",
    "    QueryGrouping,\n",
    "    ExportType,\n",
    "    TimeframeType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01bf2b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"\")\n",
    "dbutils.widgets.text(\"schema\", \"\")\n",
    "dbutils.widgets.text(\"overlap_days\", \"3\")\n",
    "dbutils.widgets.text(\"subscription_id\", \"\")\n",
    "dbutils.widgets.text(\"scope\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a33791c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "overlap_days = int(dbutils.widgets.get(\"overlap_days\") or \"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a432e8d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Disable Azure SDK verbose logs\n",
    "logging.getLogger(\"azure\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"azure.core\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"azure.identity\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85bf4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "audit_table = f\"{catalog}.{schema}.dbspend360_audit_log\"\n",
    "target_table = f\"{catalog}.{schema}.dbspend360_cloud_cost_explorer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e6b8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class AzureCostClient:\n",
    "    def __init__(self, subscription_id, tenant_id, client_id, client_secret):\n",
    "        self.subscription_id = subscription_id\n",
    "\n",
    "        self.credential = ClientSecretCredential(\n",
    "            tenant_id=tenant_id,\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret\n",
    "        )\n",
    "\n",
    "        self.client = CostManagementClient(self.credential)\n",
    "        self.scope = f\"/subscriptions/{self.subscription_id}\"\n",
    "        self.max_chunk_days = 10\n",
    "        self.max_retries = 3\n",
    "\n",
    "    # -------- Public API --------\n",
    "    def group_by_job_clusterid_daily(\n",
    "        self,\n",
    "        start_date: datetime,\n",
    "        end_date: datetime,\n",
    "        tag_name: str = \"clusterid\",\n",
    "    ):\n",
    "        \"\"\"Entry point: handles chunking and unions all results.\"\"\"\n",
    "        start_utc, end_utc = self._to_utc(start_date, end_date)\n",
    "\n",
    "        chunks = self._build_chunks(start_utc, end_utc, self.max_chunk_days)\n",
    "\n",
    "        all_chunk_dfs = []\n",
    "        for chunk_start, chunk_end in chunks:\n",
    "            print(f\"Querying chunk {chunk_start} → {chunk_end}\")\n",
    "            df = self._query_with_retries(chunk_start, chunk_end, tag_name)\n",
    "            if df is not None and df.limit(1).count() > 0:\n",
    "                all_chunk_dfs.append(df)\n",
    "            # small pause between chunks to avoid bursty usage\n",
    "            time.sleep(5)\n",
    "\n",
    "        if not all_chunk_dfs:\n",
    "            return None\n",
    "\n",
    "        # Union all chunks\n",
    "        result_df = all_chunk_dfs[0]\n",
    "        for df in all_chunk_dfs[1:]:\n",
    "            result_df = result_df.unionByName(df, allowMissingColumns=True)\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    # -------- Helpers: date & chunks --------\n",
    "    def _to_utc(self, start_date: datetime, end_date: datetime):\n",
    "        start_utc = start_date.astimezone(timezone.utc)\n",
    "        end_utc = end_date.astimezone(timezone.utc)\n",
    "        return start_utc, end_utc\n",
    "\n",
    "    def _build_chunks(self, start_utc: datetime, end_utc: datetime, max_days: int):\n",
    "        \"\"\"Return list of (chunk_start, chunk_end) in UTC.\"\"\"\n",
    "        chunks = []\n",
    "        current = start_utc\n",
    "        while current <= end_utc:\n",
    "            chunk_end = min(current + timedelta(days=max_days - 1), end_utc)\n",
    "            chunks.append((current, chunk_end))\n",
    "            current = chunk_end + timedelta(days=1)\n",
    "        return chunks\n",
    "\n",
    "    # -------- Helpers: query construction --------\n",
    "    def _build_dataset(self, tag_name: str):\n",
    "        return QueryDataset(\n",
    "            granularity=\"Daily\",\n",
    "            aggregation={\"totalCost\": QueryAggregation(name=\"Cost\", function=\"Sum\")},\n",
    "            grouping=[QueryGrouping(type=\"TagKey\", name=tag_name)],\n",
    "        )\n",
    "\n",
    "    def _build_query_definition(self, start_utc: datetime, end_utc: datetime, dataset):\n",
    "        return QueryDefinition(\n",
    "            type=ExportType.ACTUAL_COST,\n",
    "            timeframe=TimeframeType.CUSTOM,\n",
    "            time_period=QueryTimePeriod(from_property=start_utc, to=end_utc),\n",
    "            dataset=dataset,\n",
    "        )\n",
    "\n",
    "    def _build_query_body_json(self, start_utc: datetime, end_utc: datetime, tag_name: str):\n",
    "        body = {\n",
    "            \"type\": \"ActualCost\",\n",
    "            \"timeframe\": \"Custom\",\n",
    "            \"timePeriod\": {\n",
    "                \"from\": start_utc.isoformat(),\n",
    "                \"to\": end_utc.isoformat(),\n",
    "            },\n",
    "            \"dataset\": {\n",
    "                \"granularity\": \"Daily\",\n",
    "                \"aggregation\": {\n",
    "                    \"totalCost\": {\n",
    "                        \"name\": \"Cost\",\n",
    "                        \"function\": \"Sum\",\n",
    "                    }\n",
    "                },\n",
    "                \"grouping\": [\n",
    "                    {\"type\": \"TagKey\", \"name\": tag_name}\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "        return json.dumps(body)\n",
    "\n",
    "    # -------- Core call with retries --------\n",
    "    def _query_with_retries(self, start_utc, end_utc, tag_name):\n",
    "        dataset = self._build_dataset(tag_name)\n",
    "        query = self._build_query_definition(start_utc, end_utc, dataset)\n",
    "        query_json = self._build_query_body_json(start_utc, end_utc, tag_name)\n",
    "\n",
    "        attempt = 0\n",
    "        last_exception = None\n",
    "\n",
    "        while attempt < self.max_retries:\n",
    "            try:\n",
    "                return self._execute_query(query, query_json)\n",
    "            except Exception as e:\n",
    "                last_exception = e\n",
    "                attempt += 1\n",
    "\n",
    "                # crude 429 detection from SDK exception text\n",
    "                is_429 = \"429\" in str(e) or \"Too many requests\" in str(e)\n",
    "                if attempt >= self.max_retries:\n",
    "                    break\n",
    "\n",
    "                if is_429:\n",
    "                    # back off more aggressively on 429\n",
    "                    wait_sec = 30 * attempt\n",
    "                    print(f\"Rate limited (429) on main query, waiting {wait_sec}s (attempt {attempt})...\")\n",
    "                    time.sleep(wait_sec)\n",
    "                else:\n",
    "                    wait_sec = 2 ** attempt\n",
    "                    print(f\"Error on main query, waiting {wait_sec}s (attempt {attempt})...\")\n",
    "                    time.sleep(wait_sec)\n",
    "\n",
    "        raise last_exception\n",
    "\n",
    "    # -------- Single query + pagination --------\n",
    "    def _execute_query(self, query, query_json: str):\n",
    "        result = self.client.query.usage(self.scope, parameters=query)\n",
    "\n",
    "        if not result.rows:\n",
    "            return None\n",
    "\n",
    "        all_rows = list(result.rows)\n",
    "\n",
    "        next_link = getattr(result, \"next_link\", None)\n",
    "        token = None\n",
    "        if next_link:\n",
    "            token = self.credential.get_token(\n",
    "                \"https://management.azure.com/.default\"\n",
    "            ).token\n",
    "\n",
    "        while next_link:\n",
    "            next_link, page_rows = self._fetch_next_page(next_link, token, query_json)\n",
    "            all_rows.extend(page_rows)\n",
    "            if next_link:\n",
    "                time.sleep(2)\n",
    "\n",
    "        return self._rows_to_df(all_rows)\n",
    "\n",
    "    def _fetch_next_page(self, next_link: str, token: str, query_json: str):\n",
    "        while True:\n",
    "            resp = requests.post(\n",
    "                next_link,\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {token}\",\n",
    "                    \"Content-Type\": \"application/json\",\n",
    "                },\n",
    "                data=query_json,\n",
    "            )\n",
    "\n",
    "            if resp.status_code == 429:\n",
    "                # Cost Management exposes specific retry headers\n",
    "                headers = resp.headers\n",
    "                retry_after = (\n",
    "                    headers.get(\"x-ms-ratelimit-microsoft.costmanagement-qpu-retry-after\")\n",
    "                    or headers.get(\"x-ms-ratelimit-microsoft.costmanagement-entity-retry-after\")\n",
    "                    or headers.get(\"x-ms-ratelimit-microsoft.costmanagement-tenant-retry-after\")\n",
    "                    or headers.get(\"x-ms-ratelimit-microsoft.costmanagement-client-retry-after\")\n",
    "                    or headers.get(\"Retry-After\")\n",
    "                )\n",
    "\n",
    "                wait_sec = int(retry_after) if retry_after is not None else 30\n",
    "                print(f\"429 throttled, waiting {wait_sec}s before retrying nextLink...\")\n",
    "                time.sleep(wait_sec)\n",
    "                continue\n",
    "\n",
    "\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            props = data.get(\"properties\", {})\n",
    "            page_rows = props.get(\"rows\", [])\n",
    "            new_next_link = props.get(\"nextLink\")\n",
    "            return new_next_link, page_rows\n",
    "\n",
    "    # -------- Helper: convert rows to DataFrame --------\n",
    "    def _rows_to_df(self, rows):\n",
    "        return spark.createDataFrame(\n",
    "            rows,\n",
    "            [\"cost\", \"date_key\", \"tag_key\", \"cluster_id\", \"currency\"],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e2ce7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# APP\n",
    "# =======================================================\n",
    "class AzureCostReporterApp:\n",
    "\n",
    "    def __init__(self):\n",
    "        scope=dbutils.widgets.get(\"scope\")\n",
    "        subscription_id=dbutils.widgets.get(\"subscription_id\")\n",
    "        tenant_id=dbutils.secrets.get(scope, \"tenant_id\")\n",
    "        client_id=dbutils.secrets.get(scope, \"client_id\")\n",
    "        client_secret= dbutils.secrets.get(scope, \"client_secret\")\n",
    "        \n",
    "        self.client = AzureCostClient(\n",
    "            subscription_id,\n",
    "            tenant_id,\n",
    "            client_id,\n",
    "            client_secret,\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        wm = (\n",
    "            spark.table(audit_table)\n",
    "                 .filter(\"table_name = 'dbspend360_cloud_cost_explorer' AND status = 'SUCCESS'\")\n",
    "        )\n",
    "\n",
    "        if wm.limit(1).count() == 0:\n",
    "            last_end_date = datetime.now(timezone.utc).date() - timedelta(days=365-overlap_days)\n",
    "        else:\n",
    "            last_end_date = wm.agg(F.max(\"end_date\")).collect()[0][0]\n",
    "\n",
    "        start_dt = last_end_date - timedelta(days=overlap_days - 1)\n",
    "        end_dt = datetime.now(timezone.utc).date()\n",
    "\n",
    "        print(f\"Querying Azure cost from {start_dt} to {end_dt} (overlap_days={overlap_days})\")\n",
    "\n",
    "        if start_dt > end_dt:\n",
    "            message = (\n",
    "                f\"Invalid date window: start_dt={start_dt} > end_dt={end_dt}. \"\n",
    "                f\"Check audit table and overlap_days={overlap_days}.\"\n",
    "            )\n",
    "\n",
    "            run_log_df = spark.createDataFrame([\n",
    "                Row(\n",
    "                    table_name=\"dbspend360_cloud_cost_explorer\",\n",
    "                    start_date=start_dt,\n",
    "                    end_date=end_dt,\n",
    "                    status=\"FAILED\",\n",
    "                    row_count=0,\n",
    "                    message=message,\n",
    "                    created_at=datetime.now(timezone.utc),\n",
    "                )\n",
    "            ])\n",
    "            run_log_df.write.mode(\"append\").insertInto(audit_table)\n",
    "            dbutils.notebook.exit(\"FAILED: Invalid date window.\")\n",
    "\n",
    "        spark_df = self.client.group_by_job_clusterid_daily(\n",
    "            start_date=datetime.combine(start_dt, datetime.min.time(), tzinfo=timezone.utc),\n",
    "            end_date=datetime.combine(end_dt, datetime.max.time(), tzinfo=timezone.utc),\n",
    "            tag_name=\"clusterid\",\n",
    "        )\n",
    "\n",
    "        if spark_df is None or spark_df.limit(1).count() == 0:\n",
    "            print(\"No Azure cost data returned by API for the requested range.\")\n",
    "            merged_row_count = 0\n",
    "        else:\n",
    "            spark_df = spark_df.withColumn(\n",
    "                \"cost_incurred_date\",\n",
    "                F.to_date(F.col(\"date_key\").cast(\"string\"), \"yyyyMMdd\"),\n",
    "            )\n",
    "\n",
    "            inc_df = (\n",
    "                spark_df\n",
    "                .filter((F.col(\"cluster_id\").isNotNull()) & (F.col(\"cluster_id\") != \"\"))\n",
    "                .filter(F.col(\"cost_incurred_date\").isNotNull())\n",
    "            )\n",
    "\n",
    "            if inc_df.limit(1).count() == 0:\n",
    "                print(\"No incremental rows after filtering by cluster_id and cost_incurred_date.\")\n",
    "                merged_row_count = 0\n",
    "            else:\n",
    "                agg_df = (\n",
    "                    inc_df\n",
    "                    .groupBy(\"cluster_id\", \"currency\", \"cost_incurred_date\")\n",
    "                    .agg(F.sum(\"cost\").alias(\"cloud_cost\"))\n",
    "                    .withColumn(\"created_at\", F.current_timestamp())\n",
    "                    .withColumn(\"updated_at\", F.current_timestamp())\n",
    "                )\n",
    "\n",
    "                merged_row_count = agg_df.count()\n",
    "\n",
    "                agg_df.createOrReplaceTempView(\"cloud_cost_inc\")\n",
    "\n",
    "                spark.sql(f\"\"\"\n",
    "                MERGE INTO {target_table} AS t\n",
    "                USING cloud_cost_inc AS s\n",
    "                ON  t.cluster_id = s.cluster_id\n",
    "                AND t.currency = s.currency\n",
    "                AND t.cost_incurred_date = s.cost_incurred_date\n",
    "                WHEN MATCHED THEN\n",
    "                  UPDATE SET\n",
    "                    t.cloud_cost = s.cloud_cost,\n",
    "                    t.updated_at = current_timestamp()\n",
    "                WHEN NOT MATCHED THEN\n",
    "                  INSERT (cluster_id, cloud_cost, currency, cost_incurred_date, created_at, updated_at)\n",
    "                  VALUES (s.cluster_id, s.cloud_cost, s.currency, s.cost_incurred_date,\n",
    "                          current_timestamp(), current_timestamp())\n",
    "                \"\"\")\n",
    "\n",
    "        print(f\"Merged {merged_row_count} rows into {target_table} for {start_dt} → {end_dt} (overlap_days={overlap_days}).\")\n",
    "\n",
    "        # SUCCESS ENTRY (REQUIRED FOR INCREMENTAL)\n",
    "        run_log_df = spark.createDataFrame([\n",
    "            Row(\n",
    "                table_name=\"dbspend360_cloud_cost_explorer\",\n",
    "                start_date=start_dt,\n",
    "                end_date=end_dt,\n",
    "                status=\"SUCCESS\",\n",
    "                row_count=int(merged_row_count),\n",
    "                message=f\"overlap_days={overlap_days}\",\n",
    "                created_at=datetime.now(timezone.utc),\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        run_log_df.write.mode(\"append\").insertInto(audit_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d4d1b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Execute\n",
    "# =======================================================\n",
    "app = AzureCostReporterApp()\n",
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
