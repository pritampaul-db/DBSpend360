{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a940709a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7406cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"DBUCostReporter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326d8d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"\", \"CATALOG\")\n",
    "dbutils.widgets.text(\"schema\", \"\", \"SCHEMA\")\n",
    "dbutils.widgets.text(\"overlap_days\", \"3\", \"Overlap days (min 2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea0527",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# DBU Cost Client\n",
    "# =======================================================\n",
    "class DBUCostClient:\n",
    "\n",
    "    def __init__(self, audit_table: str, target_table: str, overlap_days: int):\n",
    "        self.audit_table = audit_table\n",
    "        self.target_table = target_table\n",
    "        self.overlap_days = overlap_days\n",
    "        # self.workspace_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\")\n",
    "\n",
    "    def _get_date_window(self):\n",
    "        wm = (\n",
    "            spark.table(self.audit_table)\n",
    "                 .filter(\"table_name = 'dbspend360_dbu_cost' AND status = 'SUCCESS'\")\n",
    "        )\n",
    "\n",
    "        if wm.limit(1).count() == 0:\n",
    "            last_end_date = datetime.now(timezone.utc).date() - timedelta(days=365 - self.overlap_days)\n",
    "        else:\n",
    "            last_end_date = wm.agg(F.max(\"end_date\")).collect()[0][0]\n",
    "\n",
    "        start_dt = last_end_date - timedelta(days=self.overlap_days - 1)\n",
    "        end_dt = datetime.now(timezone.utc).date()\n",
    "\n",
    "        return start_dt, end_dt\n",
    "\n",
    "    def _log_run(self, start_dt, end_dt, status, row_count, message=\"\"):\n",
    "        run_log_df = spark.createDataFrame([\n",
    "            Row(\n",
    "                table_name=\"dbspend360_dbu_cost\",\n",
    "                start_date=start_dt,\n",
    "                end_date=end_dt,\n",
    "                status=status,\n",
    "                row_count=int(row_count),\n",
    "                message=message,\n",
    "                created_at=datetime.now(timezone.utc)\n",
    "            )\n",
    "        ])\n",
    "        run_log_df.write.mode(\"append\").insertInto(self.audit_table)\n",
    "\n",
    "    def compute_and_merge_dbu_cost(self):\n",
    "        start_dt, end_dt = self._get_date_window()\n",
    "\n",
    "        if start_dt > end_dt:\n",
    "            message = f\"Invalid date window: start_dt={start_dt} > end_dt={end_dt}.\"\n",
    "            logger.error(message)\n",
    "            self._log_run(start_dt, end_dt, \"FAILED\", 0, message)\n",
    "            dbutils.notebook.exit(\"FAILED: Invalid date window.\")\n",
    "\n",
    "        logger.info(f\"Loading DBU cost from {start_dt} to {end_dt}\")\n",
    "\n",
    "        # 1. job clusters\n",
    "        cluster_df = (\n",
    "            spark.table(\"system.compute.clusters\")\n",
    "                 .select(\"cluster_id\", \"cluster_name\", \"cluster_source\", \"workspace_id\")\n",
    "                 .filter(\"cluster_source = 'JOB'\")\n",
    "        )\n",
    "\n",
    "        # 2. usage + list_prices, filtered by usage_date window\n",
    "        usage_df = (\n",
    "            spark.table(\"system.billing.usage\")\n",
    "                 .alias(\"usage\")\n",
    "                #  .filter(F.col(\"usage.workspace_id\") == workspace_id)\n",
    "                 .filter((F.col(\"usage.usage_date\") >= F.lit(start_dt)) &\n",
    "                         (F.col(\"usage.usage_date\") <= F.lit(end_dt)))\n",
    "        )\n",
    "\n",
    "        list_prices_df = spark.table(\"system.billing.list_prices\").alias(\"list_prices\")\n",
    "\n",
    "        df = (\n",
    "            usage_df.join(\n",
    "                list_prices_df,\n",
    "                on=(\n",
    "                    (F.col(\"usage.sku_name\") == F.col(\"list_prices.sku_name\")) &\n",
    "                    (F.col(\"usage.usage_start_time\") >= F.col(\"list_prices.price_start_time\")) &\n",
    "                    (\n",
    "                        (F.col(\"usage.usage_start_time\") < F.col(\"list_prices.price_end_time\")) |\n",
    "                        F.col(\"list_prices.price_end_time\").isNull()\n",
    "                    )\n",
    "                ),\n",
    "                how=\"left\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 3. filter to job runs and aggregate\n",
    "        filtered_df = df.filter(\n",
    "            F.col(\"usage.usage_metadata\")[\"job_run_id\"].isNotNull()\n",
    "        )\n",
    "\n",
    "        #updated to keep sku names as sku1+sku2+sku3 in case of multiple sku type for same cluster\n",
    "        agg_df = (\n",
    "            filtered_df\n",
    "            .groupBy(\n",
    "                F.col(\"usage.usage_metadata\")[\"cluster_id\"].alias(\"job_cluster_id\"),\n",
    "                F.col(\"usage.usage_metadata\")[\"job_id\"].alias(\"job_id\"),\n",
    "                F.col(\"usage.usage_metadata\")[\"job_run_id\"].alias(\"run_id\"),\n",
    "                F.col(\"usage.usage_date\").alias(\"usage_date\"),\n",
    "                F.col(\"usage.workspace_id\").alias(\"workspace_id\")\n",
    "            )\n",
    "            .agg(\n",
    "                F.sum(\n",
    "                    F.col(\"usage.usage_quantity\")\n",
    "                    * F.col(\"list_prices.pricing\")[\"default\"].cast(\"double\")\n",
    "                ).alias(\"databricks_cost\"),\n",
    "                F.concat_ws(\n",
    "                    \" + \",\n",
    "                    F.array_sort(F.collect_set(F.col(\"usage.sku_name\")))\n",
    "                ).alias(\"sku_name_merged\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 4. join with job clusters\n",
    "        job_cluster_df = (\n",
    "            cluster_df\n",
    "            .select(\"cluster_id\")\n",
    "            .dropDuplicates([\"cluster_id\"])\n",
    "        )\n",
    "\n",
    "        joined_df = (\n",
    "            agg_df.join(\n",
    "                job_cluster_df,\n",
    "                on=(agg_df[\"job_cluster_id\"] == job_cluster_df[\"cluster_id\"]),\n",
    "                how=\"inner\"\n",
    "            )\n",
    "            .drop(\"job_cluster_id\")\n",
    "        )\n",
    "\n",
    "        joined_df = joined_df.withColumn(\"currency\", F.lit(\"USD\"))\n",
    "\n",
    "        # 5. prepare incremental dataframe\n",
    "        dbu_inc_df = (\n",
    "            joined_df\n",
    "            .select(\n",
    "                \"cluster_id\",\n",
    "                \"job_id\",\n",
    "                \"run_id\",\n",
    "                \"usage_date\",\n",
    "                \"databricks_cost\",\n",
    "                \"currency\",\n",
    "                F.col(\"sku_name_merged\").alias(\"sku_name\"),\n",
    "                \"workspace_id\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if dbu_inc_df.limit(1).count() == 0:\n",
    "            logger.info(\"No DBU rows after filtering / aggregation.\")\n",
    "            merged_row_count = 0\n",
    "        else:\n",
    "            merged_row_count = dbu_inc_df.count()\n",
    "\n",
    "            dbu_inc_df = (\n",
    "                dbu_inc_df\n",
    "                .withColumn(\"created_at\", F.current_timestamp())\n",
    "                .withColumn(\"updated_at\", F.current_timestamp())\n",
    "            )\n",
    "\n",
    "            dbu_inc_df.createOrReplaceTempView(\"dbu_cost_inc\")\n",
    "\n",
    "            # 6. MERGE into target\n",
    "            spark.sql(f\"\"\"\n",
    "            MERGE INTO {self.target_table} AS t\n",
    "            USING dbu_cost_inc AS s\n",
    "            ON  t.cluster_id = s.cluster_id\n",
    "            AND t.job_id     = s.job_id\n",
    "            AND t.run_id     = s.run_id\n",
    "            AND t.usage_date = s.usage_date\n",
    "            WHEN MATCHED THEN\n",
    "              UPDATE SET\n",
    "                t.databricks_cost = s.databricks_cost,\n",
    "                t.updated_at      = current_timestamp()\n",
    "            WHEN NOT MATCHED THEN\n",
    "              INSERT (cluster_id, job_id, run_id, usage_date,\n",
    "                      databricks_cost, currency, sku_name, workspace_id,\n",
    "                      created_at, updated_at)\n",
    "              VALUES (s.cluster_id, s.job_id, s.run_id, s.usage_date,\n",
    "                      s.databricks_cost, s.currency, s.sku_name, s.workspace_id,\n",
    "                      current_timestamp(), current_timestamp());\n",
    "            \"\"\")\n",
    "\n",
    "        # 7. append run log\n",
    "        self._log_run(start_dt, end_dt, \"SUCCESS\", merged_row_count, \"\")\n",
    "        logger.info(f\"Merged {merged_row_count} rows into {self.target_table} for {start_dt} â†’ {end_dt}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b4cf7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# APP\n",
    "# =======================================================\n",
    "class DBUCostReporterApp:\n",
    "\n",
    "    def __init__(self):\n",
    "        catalog = dbutils.widgets.get(\"catalog\")\n",
    "        schema = dbutils.widgets.get(\"schema\")\n",
    "        overlap_days = int(dbutils.widgets.get(\"overlap_days\") or \"2\")\n",
    "\n",
    "        if overlap_days < 2:\n",
    "            logger.warning(\"overlap_days < 2; forcing to 2 for cost convergence best practice.\")\n",
    "            overlap_days = 2\n",
    "\n",
    "        audit_table = f\"{catalog}.{schema}.dbspend360_audit_log\"\n",
    "        target_table = f\"{catalog}.{schema}.dbspend360_dbu_cost\"\n",
    "\n",
    "        self.client = DBUCostClient(\n",
    "            audit_table=audit_table,\n",
    "            target_table=target_table,\n",
    "            overlap_days=overlap_days,\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        self.client.compute_and_merge_dbu_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb93f0c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Execute\n",
    "# =======================================================\n",
    "app = DBUCostReporterApp()\n",
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
